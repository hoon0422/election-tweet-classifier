{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Election Classifier",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "tM1UjB0w_AVc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a5a364c9-7fab-4275-a9ff-8e6adc895f5b"
      },
      "source": [
        "!pip install torchtext==0.6"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torchtext==0.6 in /usr/local/lib/python3.6/dist-packages (0.6.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from torchtext==0.6) (4.41.1)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (from torchtext==0.6) (1.7.0+cu101)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.6/dist-packages (from torchtext==0.6) (0.1.94)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from torchtext==0.6) (2.23.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torchtext==0.6) (1.18.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from torchtext==0.6) (1.15.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.6/dist-packages (from torch->torchtext==0.6) (3.7.4.3)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch->torchtext==0.6) (0.16.0)\n",
            "Requirement already satisfied: dataclasses in /usr/local/lib/python3.6/dist-packages (from torch->torchtext==0.6) (0.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext==0.6) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext==0.6) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext==0.6) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext==0.6) (2020.11.8)\n",
            "\u001b[31mERROR: Could not find a version that satisfies the requirement Zipfile (from versions: none)\u001b[0m\n",
            "\u001b[31mERROR: No matching distribution found for Zipfile\u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ta6LsoyyOIlW"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torchtext\n",
        "from torchtext.vocab import FastText\n",
        "from torchtext.vocab import GloVe\n",
        "from torchtext.data import Field\n",
        "from torchtext.data import Dataset, Example\n",
        "from torchtext.data import BucketIterator\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "import re\n",
        "import matplotlib.pyplot as plt\n",
        "import json"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lxkCYUshN2oS",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 37
        },
        "outputId": "547aee98-d380-48b0-9d5f-d7b6822e2e1c"
      },
      "source": [
        "torchtext.__version__"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'0.6.0'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DzfFVV1u_6KV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1e972adf-6906-4691-cb40-1ed520d3b09a"
      },
      "source": [
        "word_to_ix = {\"hello\": 0, \"world\": 1}\n",
        "embeds = nn.Embedding(2, 5)  # 2 words in vocab, 5 dimensional embeddings\n",
        "lookup_tensor = torch.tensor([word_to_ix[\"hello\"]], dtype=torch.long)\n",
        "hello_embed = embeds(lookup_tensor)\n",
        "print(hello_embed)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[-0.8016, -0.8072, -0.1534,  0.7872, -0.0733]],\n",
            "       grad_fn=<EmbeddingBackward>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oR7acrFqL-_R",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b7e3ea62-2b5c-417b-9c5a-a202f64e4aef"
      },
      "source": [
        "!ls '/content/drive/My Drive/490A Project/'"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " Biden\t'Final Dataset'   Trump\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Piq5X1xUC8Ah",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8e3a5d4d-c663-4ec2-ee39-3c755d8390ab"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "biden_data_dir = '/content/drive/My Drive/490A Project/Biden'\n",
        "trump_data_dir = '/content/drive/My Drive/490A Project/Trump'\n",
        "biden_folders = os.listdir(biden_data_dir)\n",
        "trump_folders = os.listdir(trump_data_dir)\n",
        "classes = [\"BIDEN\", \"TRUMP\"]\n",
        "print(\"Selected Biden Hashtags:\", biden_folders)\n",
        "print(\"Selected Trump Hashtags:\",trump_folders)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n",
            "Selected Biden Hashtags: ['Icon\\r', 'BidenHarris', 'Biden2020']\n",
            "Selected Trump Hashtags: ['Icon\\r', 'TrumpPence', 'Trump2020']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A5FPcY2gwTUr"
      },
      "source": [
        "import zipfile\n",
        "\n",
        "final_data = '/content/drive/My Drive/490A Project/Final Dataset/final_dataset_no_error.zip'\n",
        "biden_final_dir = \"merged_no_dups_no_error/biden\"\n",
        "trump_final_dir = \"merged_no_dups_no_error/trump\"\n",
        "\n",
        "with zipfile.ZipFile(final_data, 'r') as zipObj:\n",
        "   # Extract all the contents of zip file in current directory\n",
        "   zipObj.extractall()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VP8LLkC_zKBE",
        "outputId": "25c0ea40-c74f-4e42-e0ed-5628c63a2e42"
      },
      "source": [
        "!ls \"merged_no_dups_no_error/\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "biden  trump\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iLLrIGRtV6wV"
      },
      "source": [
        "# Only Preprocessing needed is removing hashtags, removing keywords, remove users and replacing links\n",
        "# Lemmatising and Stemming is not necessary since word embeddings are being used\n",
        "# The tokenising is done by TorchText at the time of creating the field\n",
        "# Converting to lowercase is done by TorchText at the time of creating the field\n",
        "\n",
        "URL_replacement = \"[link]\"\n",
        "\n",
        "'''\n",
        "def preprocess(samples):\n",
        "\n",
        "  for index, sample in samples.iterrows():\n",
        "    temp = sample[\"full_text\"]\n",
        "\n",
        "    temp = re.sub('#[^a-zA-Z0-9]+', '', temp)\n",
        "    temp = re.sub('@[^a-zA-Z0-9]+', '', temp)\n",
        " \n",
        "    temp = re.sub(\"http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+\",\n",
        "                                 URL_replacement, \n",
        "                                 temp,\n",
        "                                 flags=re.MULTILINE, \n",
        "                                 )\n",
        "    \n",
        "    #samples[\"full_text\"] = samples[\"full_text\"].replace(sample[\"full_text\"],temp)\n",
        "  return temp\n",
        "'''\n",
        "\n",
        "def preprocess(text):\n",
        "  temp = text\n",
        "  temp = re.sub('#[^a-zA-Z0-9]+', '', temp)\n",
        "  temp = re.sub('@[^a-zA-Z0-9]+', '', temp)\n",
        "  temp = re.sub(\"http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+\",\n",
        "                                 URL_replacement, \n",
        "                                 temp,\n",
        "                                 flags=re.MULTILINE, \n",
        "                                 )\n",
        "  text = temp\n",
        "  return temp"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BEv9lRoYNkDi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "94fabdee-c998-443c-d2d3-08674bf78d09"
      },
      "source": [
        "TotalData = { classes[0]: [],\n",
        "              classes[1]: []    \n",
        "}\n",
        "\n",
        "df = pd.DataFrame()\n",
        "\n",
        "'''\n",
        "#Add all data to Biden dataset\n",
        "for dir in os.listdir(biden_data_dir):\n",
        "  if dir == 'Icon\\r':\n",
        "    continue\n",
        "  for fname in os.listdir(biden_data_dir + \"/\" + dir):\n",
        "    if fname == \"Icon\\r\":\n",
        "      continue\n",
        "    value = pd.read_json(biden_data_dir + \"/\" + dir + '/' + fname)\n",
        "    if \"full_text\" not in value:\n",
        "      continue\n",
        "    value = preprocess(value)\n",
        "    value_text = list(value[\"full_text\"])\n",
        "    value_labels = [classes[0] for i in range(len(value_text)) ]\n",
        "    temp = { \"Text\": value_text,\n",
        "             \"Class\": value_labels\n",
        "    }\n",
        "    df2 = pd.DataFrame(temp)\n",
        "    df = df.append(df2, ignore_index=True)\n",
        "\n",
        "print(\"Done Collecting Biden Data\")\n",
        "\n",
        "\n",
        "#Add all data to Trump dataset\n",
        "for dir in os.listdir(trump_data_dir):\n",
        "  if dir == 'Icon\\r':\n",
        "    continue\n",
        "  for fname in os.listdir(trump_data_dir + \"/\" + dir):\n",
        "    if fname == \"Icon\\r\":\n",
        "      continue\n",
        "    value = pd.read_json(trump_data_dir + \"/\" + dir + '/' + fname)\n",
        "    if \"full_text\" not in value:\n",
        "      continue\n",
        "    value_text = list(value[\"full_text\"])\n",
        "    value_labels = [classes[1] for i in range(len(value_text)) ]\n",
        "    temp = { \"Text\": value_text,\n",
        "             \"Class\": value_labels\n",
        "    }\n",
        "    df2 = pd.DataFrame(temp)\n",
        "    df = df.append(df2, ignore_index=True)\n",
        "print(\"Done Collecting Trump Data\")\n",
        "'''\n",
        "\n",
        "for fname in os.listdir(biden_final_dir + \"/\"):\n",
        "    if fname == \"Icon\\r\":\n",
        "      continue\n",
        "    value = data = json.load(open(biden_final_dir + \"/\" + fname, encoding='utf-8'))\n",
        "    if \"full_text\" not in value:\n",
        "      continue\n",
        "    \n",
        "    value_text = preprocess(value[\"full_text\"])\n",
        "    value_labels = [classes[0]]\n",
        "    temp = { \"Text\": value_text,\n",
        "             \"Class\": value_labels\n",
        "    }\n",
        "    df2 = pd.DataFrame(temp)\n",
        "    df = df.append(df2, ignore_index=True)\n",
        "\n",
        "print(\"Done Collecting Biden Data\")\n",
        "\n",
        "for fname in os.listdir(trump_final_dir + \"/\"):\n",
        "    if fname == \"Icon\\r\":\n",
        "      continue\n",
        "    value = data = json.load(open(trump_final_dir + \"/\" + fname, encoding='utf-8'))\n",
        "    if \"full_text\" not in value:\n",
        "      continue\n",
        "    value_text = value[\"full_text\"]\n",
        "    value_text = preprocess(value_text)\n",
        "    value_labels = [classes[1]]\n",
        "    temp = { \"Text\": value_text,\n",
        "             \"Class\": value_labels\n",
        "    }\n",
        "    df2 = pd.DataFrame(temp)\n",
        "    df = df.append(df2, ignore_index=True)\n",
        "\n",
        "print(\"Done Collecting Trump Data\")\n",
        "\n",
        "print(df)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Done Collecting Biden Data\n",
            "Done Collecting Trump Data\n",
            "                                                     Text  Class\n",
            "0       @BillStepien Bill, you've done your best on a ...  BIDEN\n",
            "1       Bring it home, America. Let's prove that we be...  BIDEN\n",
            "2       RT @meg_Y12: From an American friend receiving...  BIDEN\n",
            "3       @Timodc Yes, Collin and Denton County both.  B...  BIDEN\n",
            "4       @redshoe9 @JudithWick1 @BettyBowers @DJJudd @y...  BIDEN\n",
            "...                                                   ...    ...\n",
            "337301  Time to get the #RedWave STARRTTEEDD!!!!\\n#Tru...  TRUMP\n",
            "337302  Yâ€™all have anymore ðŸ˜³ ? Please post below ! We ...  TRUMP\n",
            "337303  RT @charlybcrawford: Making Halloween Rodeo gr...  TRUMP\n",
            "337304  ðŸ¤®ðŸ¤¢ #MediaReform is a must if we're going to #M...  TRUMP\n",
            "337305  Remember to #VOTE for #TrumpPence during this ...  TRUMP\n",
            "\n",
            "[337306 rows x 2 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HSdOBCbiNNrs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dee577cd-80f0-4c73-e5b2-6cf274e4b6ea"
      },
      "source": [
        "text_field = Field(\n",
        "    sequential=True,\n",
        "    tokenize='toktok', \n",
        "    fix_length=5,\n",
        "    lower=True\n",
        ")\n",
        "label_field = Field(sequential=False, use_vocab=False)\n",
        "# sadly have to apply preprocess manually\n",
        "preprocessed_text = df['Text'].apply(\n",
        "    lambda x: text_field.preprocess(x)\n",
        ")\n",
        "# load fastext simple embedding with 300d\n",
        "text_field.build_vocab(\n",
        "    preprocessed_text, \n",
        "    vectors='fasttext.simple.300d'\n",
        ")\n",
        "# get the vocab instance\n",
        "vocab = text_field.vocab\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            ".vector_cache/wiki.simple.vec: 293MB [00:12, 23.6MB/s]                           \n",
            "  0%|          | 0/111051 [00:00<?, ?it/s]Skipping token b'111051' with 1-dimensional vector [b'300']; likely a header\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 110691/111051 [00:11<00:00, 8707.94it/s]"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6kInxT32r5AV"
      },
      "source": [
        "embedding = FastText('simple')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o8thEdOVsE0n",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9354bfae-7407-4135-fcad-d5d384434f51"
      },
      "source": [
        "# known token, in my case print 12\n",
        "print(vocab['are'])\n",
        "# unknown token, will print 0\n",
        "print(vocab['crazy'])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "31\n",
            "1232\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yO1OP43yPeK0"
      },
      "source": [
        "ltoi = {l: i for i, l in enumerate(df['Class'].unique())}\n",
        "df['Class'] = df['Class'].apply(lambda y: ltoi[y])\n",
        "class DataFrameDataset(Dataset):\n",
        "    def __init__(self, df: pd.DataFrame, fields: list):\n",
        "        super(DataFrameDataset, self).__init__(\n",
        "            [\n",
        "                Example.fromlist(list(r), fields) \n",
        "                for i, r in df.iterrows()\n",
        "            ], \n",
        "            fields\n",
        "        )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zBz1fXhaPlCX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dbedb87b-fe67-4373-a137-c3337758a9f2"
      },
      "source": [
        "train_dataset, test_dataset = DataFrameDataset(\n",
        "    df=df, \n",
        "    fields=(\n",
        "        ('text', text_field),\n",
        "        ('label', label_field)\n",
        "    )\n",
        ").split()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\r100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 110691/111051 [00:30<00:00, 8707.94it/s]"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pwzmiteRPshQ"
      },
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(torch.cuda.current_device())\n",
        "\n",
        "train_iter, test_iter = BucketIterator.splits(\n",
        "    datasets=(train_dataset, test_dataset), \n",
        "    batch_sizes=(1000, 1000),\n",
        "    sort=False,\n",
        "    device = device\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IAPa45gGPv37"
      },
      "source": [
        "class ModelParam(object):\n",
        "    def __init__(self, param_dict: dict = dict()):\n",
        "        self.input_size = param_dict.get('input_size', 0)\n",
        "        self.vocab_size = param_dict.get('vocab_size')\n",
        "        self.embedding_dim = param_dict.get('embedding_dim', 300)\n",
        "        self.target_dim = param_dict.get('target_dim', 2)\n",
        "        \n",
        "class MyModel(nn.Module):\n",
        "    def __init__(self, model_param: ModelParam):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(\n",
        "            model_param.vocab_size, \n",
        "            model_param.embedding_dim\n",
        "        )\n",
        "        self.lin = nn.Linear(\n",
        "            model_param.input_size * model_param.embedding_dim, \n",
        "            model_param.target_dim\n",
        "        )\n",
        "        \n",
        "    def forward(self, x):\n",
        "        features = self.embedding(x).view(x.size()[0], -1)\n",
        "        features = F.relu(features)\n",
        "        features = self.lin(features)\n",
        "        return features"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BhWnD2gYVgdA"
      },
      "source": [
        "def accuracy(outputs, labels):\n",
        "    _, preds = torch.max(outputs, dim=1)\n",
        "    return torch.tensor(torch.sum(preds == labels).item() / len(preds))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kQm0bWhWPxLN"
      },
      "source": [
        "model_param = ModelParam(\n",
        "    param_dict=dict(\n",
        "        vocab_size=len(text_field.vocab),\n",
        "        input_size=5\n",
        "    )\n",
        ")\n",
        "\n",
        "model = MyModel(model_param).to(device)\n",
        "loss_function = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
        "epochs = 1000\n",
        "\n",
        "all_train_losses = list()\n",
        "all_test_losses = list()\n",
        "for epoch in range(epochs):\n",
        "    epoch_losses = list()\n",
        "    epoch_accuracies = list()\n",
        "    for batch in train_iter:\n",
        "        optimizer.zero_grad()\n",
        "        prediction = model(batch.text.T)\n",
        "        loss = loss_function(prediction, batch.label)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        epoch_accuracies.append(accuracy(prediction, batch.label))\n",
        "        epoch_losses.append(loss.item())\n",
        "    all_train_losses.append(sum(epoch_accuracies)/len(epoch_accuracies))\n",
        "    #print('train accuracy on epoch {} : {:.3f}'.format(epoch, sum(epoch_accuracies)/len(epoch_accuracies)))\n",
        "    #print('train loss on epoch {} : {:.3f}'.format(epoch, np.mean(epoch_losses)))\n",
        "    \n",
        "    test_losses = list()\n",
        "    test_accuracies = list()\n",
        "    for batch in test_iter:\n",
        "        with torch.no_grad():\n",
        "            optimizer.zero_grad()\n",
        "            prediction = model(batch.text.T)\n",
        "            loss = loss_function(prediction, batch.label)\n",
        "            \n",
        "            test_accuracies.append(accuracy(prediction, batch.label))\n",
        "            test_losses.append(loss.item())\n",
        "    all_test_losses.append(sum(test_accuracies)/len(test_accuracies))\n",
        "    #print('test accuracy on epoch {} : {:.3f}'.format(epoch, sum(test_accuracies)/len(test_accuracies)))\n",
        "    \n",
        "    #print('test loss on epoch {}: {:.3f}'.format(epoch, np.mean(test_losses)))\n",
        "    if (epoch%100 == 0):\n",
        "      print(\"Done with epoch \", epoch, \":\")\n",
        "print(\"Done!\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XoGrBMOWz2cH"
      },
      "source": [
        "epochs_list = [*range(0,epochs)]\n",
        "plt.plot(epochs_list, all_train_losses)\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.title(\"Train Losses over 1000 epochs\")\n",
        "plt.show()\n",
        "\n",
        "plt.plot(epochs_list, all_test_losses)\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.title(\"Train Losses over 1000 epochs\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}